{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un GAN (Generative Adversarial Network) est constitué de deux réseaux neuronaux qui s’affrontent dans un processus d’apprentissage :\n",
    "\n",
    "Le générateur : il crée des données synthétiques à partir d’un bruit aléatoire. Son objectif est de produire des données qui ressemblent le plus possible aux données réelles.\n",
    "Le discriminateur : il tente de distinguer les données réelles des données générées par le générateur.\n",
    "Ces deux réseaux sont en compétition :\n",
    "\n",
    "Le générateur essaie de tromper le discriminateur en produisant des données de plus en plus réalistes.\n",
    "Le discriminateur s'améliore progressivement pour mieux différencier les vraies données des fausses.\n",
    "Ce processus se poursuit jusqu’à ce que le générateur parvienne à créer des données si réalistes que le discriminateur ne puisse plus les distinguer des vraies données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CTGAN (Conditional Tabular GAN) est un modèle spécialisé dans la génération de données tabulaires synthétiques. Contrairement aux GANs classiques, il prend en compte la nature discrète et continue des variables dans un tableau, ce qui le rend plus efficace pour les bases de données structurées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from lightgbm) (2.2.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (from lightgbm) (1.15.2)\n",
      "Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 19:49:57.644711: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-05 19:49:57.649234: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-05 19:49:57.661342: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741204197.680916   11573 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741204197.686860   11573 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-05 19:49:57.708542: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.metrics import precision_score, recall_score, f1_score,\\\n",
    "                            #accuracy_score, balanced_accuracy_score,classification_report,\\\n",
    "                            #plot_confusion_matrix, confusion_matrix\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import lightgbm as lgb\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, Concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "np.random.seed(1635848)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cGAN():\n",
    "    \n",
    "    \"\"\"\n",
    "    \"Classe contenant 3 méthodes (et init) : générateur, discriminateur et entraînement. \n",
    "    Le générateur est entraîné en utilisant du bruit aléatoire \n",
    "    et un label comme entrées. Le discriminateur est entraîné en utilisant \n",
    "    des échantillons réels/faux et des labels comme entrées.\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,latent_dim=32, out_shape=14):\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.out_shape = out_shape \n",
    "        self.num_classes = 2\n",
    "        # Utilisation de Adam comme optimizer\n",
    "        optimizer = Adam(0.0002, 0.5) #mettre à jour les poids d'un réseau de neurones pendant l'entraînement\n",
    "        \n",
    "        # construction du discriminateur\n",
    "        self.discriminator = self.discriminator()\n",
    "        #définission de la fonction de perte, l'optimiseur et les métriques à utiliser pour l'entraînement\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "\n",
    "        # Construction du générateur\n",
    "        self.generator = self.generator()\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "         #le générateur prend à la fois le bruit aléatoire et le label comme entrée pour générer des échantillons (gen_samples)\n",
    "        gen_samples = self.generator([noise, label]) \n",
    "        \n",
    "        # désactivation de l'entraînement du discriminateur lorsque le générateur est entraîné\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        #probabilité qui indique si le discriminateur pense que l'échantillon généré est réel\n",
    "        valid = self.discriminator([gen_samples, label])\n",
    "\n",
    "        #tentative du générateur de tromper le discriminateur en produisant des échantillons aussi réalistes que possible\n",
    "\n",
    "        # combinaison des 2 modèles\n",
    "        self.combined = Model([noise, label], valid)\n",
    "        \n",
    "        self.combined.compile(loss=['binary_crossentropy'],\n",
    "                              optimizer=optimizer,\n",
    "                             metrics=['accuracy'])\n",
    "\n",
    "    def generator(self):\n",
    "        #initialise les poids des couches du générateur en utilisant une distribution normale avec une moyenne de 0 et un écart type de 0.02\n",
    "        init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "        # les couches seront empilées les unes après les autres de manière linéaire.\n",
    "        model = Sequential()\n",
    "\n",
    "        # Une couche de 128 neurones est crée\n",
    "        model.add(Dense(128, input_dim=self.latent_dim))\n",
    "        # 20% des neurones sont éteints lors de l'entrainement \n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LeakyReLU(alpha=0.2)) #fonction d'activation\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        # Ajout de nouvelles couches\n",
    "        model.add(Dense(256))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        model.add(Dense(512))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        #couche finale\n",
    "        model.add(Dense(self.out_shape, activation='tanh'))\n",
    "\n",
    "        #création du bruit et du label conditionnel\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        #transforme les labels en vecteurs d'embedding de taille latent_dim\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "        \n",
    "        model_input = multiply([noise, label_embedding])\n",
    "        gen_sample = model(model_input)\n",
    "\n",
    "        return Model([noise, label], gen_sample, name=\"Generator\")\n",
    "\n",
    "    #De même on crée le discriminateur\n",
    "\n",
    "    def discriminator(self):\n",
    "        init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512, input_dim=self.out_shape, kernel_initializer=init))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Dense(256, kernel_initializer=init))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "        model.add(Dense(128, kernel_initializer=init))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "        #dernière couche a 1 neurone\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        gen_sample = Input(shape=(self.out_shape,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.out_shape)(label))\n",
    "        model_input = multiply([gen_sample, label_embedding])\n",
    "\n",
    "        #donne score entre 0 et 1\n",
    "        validity = model(model_input)\n",
    "\n",
    "        return Model(inputs=[gen_sample, label], outputs=validity, name=\"Discriminator\")\n",
    "    \n",
    "\n",
    "    def train(self, X_train, y_train, pos_index, neg_index, epochs, sampling=False, batch_size=32, sample_interval=100, plot=True): \n",
    "        \n",
    "       #Les listes vont stocker les pertes du générateur et du discriminateur à chaque époque\n",
    "        global G_losses\n",
    "        global D_losses\n",
    "        \n",
    "        G_losses = []\n",
    "        D_losses = []\n",
    "        # les échantillons réels, utilisés pour entraîner le discriminateur\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        #échantillons générés, utilisés pour entraîner le discriminateur considérs comme faux\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # si sampling==True --> entrainer le  discriminator avec un batch de 8 échantillons de  postivite class et le reste negative class\n",
    "            if sampling:\n",
    "                idx1 = np.random.choice(pos_index, 8) #échantillon réel pour le discriminateur\n",
    "                idx0 = np.random.choice(neg_index, batch_size-8) #échantillon faux ou générés\n",
    "                idx = np.concatenate((idx1, idx0))\n",
    "\n",
    "            # if sampling!=True --> les échantillons sont choisis aléatoirement parmi l'ensemble d'entraînement pour créer un batch de taille 32\n",
    "            else:\n",
    "                idx = np.random.choice(len(y_train), batch_size)\n",
    "\n",
    "            #récupération des échantillons et des labels à partir des indices\n",
    "            samples, labels = X_train[idx], y_train[idx]\n",
    "            # mélange\n",
    "            samples, labels = shuffle(samples, labels)\n",
    "            \n",
    "            # Génération du bruit\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            #génération de échantillons \"faux\" par le générateur\n",
    "            gen_samples = self.generator.predict([noise, labels])\n",
    "\n",
    "            # label smoothing: les labels des échantillons réels sont légèrement modifiés pour les rendre moins parfaits, et les labels des échantillons générés sont légèrement améliorés\n",
    "            # pour éviter un apprentissage trop rapide et éviter que le modèle ne devienne trop confiant\n",
    "            if epoch < epochs//1.5:\n",
    "                valid_smooth = (valid+0.1)-(np.random.random(valid.shape)*0.1)\n",
    "                fake_smooth = (fake-0.1)+(np.random.random(fake.shape)*0.1)\n",
    "            else:\n",
    "                valid_smooth = valid \n",
    "                fake_smooth = fake\n",
    "                \n",
    "            # Entraînement du discriminateur\n",
    "            self.discriminator.trainable = True #activation de l'entraînement du disciminateur\n",
    "            d_loss_real = self.discriminator.train_on_batch([samples, labels], valid_smooth) # Entraîner sur les échantillons réels\n",
    "            d_loss_fake = self.discriminator.train_on_batch([gen_samples, labels], fake_smooth) # Entraîner sur les échantillons générés\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) # Calculer la perte globale du discriminateur: moyenne des pertes réelles et générées, pondérée par 0.5\n",
    "\n",
    " \n",
    "         #d_loss_real : La perte pour les échantillons réels est calculée en fonction des valid_smooth (les labels lissés des échantillons réels).\n",
    "#d_loss_fake : La perte pour les échantillons générés est calculée en fonction des fake_smooth (les labels lissés des échantillons générés).\"\n",
    "\n",
    "\n",
    "            # Entrainement du générateur\n",
    "            self.discriminator.trainable = False #désactive l'entraînement du discriminateur\n",
    "            sampled_labels = np.random.randint(0,40, batch_size).reshape(-1, 1)\n",
    "            # On génère des labels aléatoires (0 ou pour l'entraînement du générateur, afin qu'il apprenne à produire des échantillons qui trompent le discriminateur\n",
    "            \n",
    "            g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n",
    "\n",
    "          #Afficher les pertes\n",
    "            if (epoch+1)%sample_interval==0:\n",
    "                print('[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n",
    "                  % (epoch, epochs, d_loss[0], g_loss[0]))\n",
    "            G_losses.append(g_loss[0])\n",
    "            D_losses.append(d_loss[0])\n",
    "            if plot:\n",
    "                if epoch+1==epochs:\n",
    "                    plt.figure(figsize=(10,5))\n",
    "                    plt.title(\"Generator and Discriminator Loss\")\n",
    "                    plt.plot(G_losses,label=\"G\")\n",
    "                    plt.plot(D_losses,label=\"D\")\n",
    "                    plt.xlabel(\"iterations\")\n",
    "                    plt.ylabel(\"Loss\")\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
